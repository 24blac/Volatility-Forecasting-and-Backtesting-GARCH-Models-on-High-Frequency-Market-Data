{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGrbVN1lKNhv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from arch import arch_model\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import pandas_ta as ta\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from a CSV file and parse the 'time' column as a datetime object\n",
        "data = pd.read_csv(\"../../../Documents/DerivData/Boom 1000 Index_m1.csv\", index_col=0)\n",
        "data[\"time\"] = pd.to_datetime(data[\"time\"], unit=\"s\")"
      ],
      "metadata": {
        "id": "QzaWBhXeKXcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data for the months between June and August (6 to 8)\n",
        "data[\"month\"] = data[\"time\"].dt.month\n",
        "data = data[(data[\"month\"] > 5) & (data[\"month\"] < 9)].copy()\n",
        "\n",
        "# Drop irrelevant columns: tick_volume, spread, real_volume\n",
        "data.drop(columns = [\"tick_volume\", \"spread\", \"real_volume\"], inplace=True)\n",
        "\n",
        "# Further filter data for July and restrict to the first 6 days\n",
        "data = data[data[\"month\"]==7]\n",
        "data = data[data[\"time\"].dt.day < 7].reset_index(drop=True)\n",
        "\n",
        "# Display the first few rows of the filtered data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "uFPX6gH6Kyjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate returns as percentage change in closing prices\n",
        "data[\"returns\"] = 100 * data[\"close\"].pct_change().dropna()\n",
        "returns = 100 * data[\"close\"].pct_change().dropna()\n",
        "\n",
        "# Calculate log returns\n",
        "log_returns = np.log(data[\"close\"]).diff().dropna()\n",
        "\n",
        "# Plot histogram of log returns\n",
        "plt.figure(figsize=(10,4))\n",
        "log_returns.hist(bins=30)\n",
        "plt.ylim(0, 125)\n"
      ],
      "metadata": {
        "id": "wZo2SdTsK2xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot log returns over time\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(log_returns)\n",
        "plt.ylabel(\"Pct Change\", fontsize=16)\n",
        "plt.title(\"Boom 1000 Index Returns\", fontsize=20)\n"
      ],
      "metadata": {
        "id": "DRum9ctSK8kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ACF and PACF of squared log returns to evaluate volatility clustering\n",
        "plot_acf(log_returns**2)\n",
        "plt.ylim(-0.25, 0.25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TOFB7hzcLBes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pacf(log_returns**2)\n",
        "plt.ylim(-0.25, 0.25)"
      ],
      "metadata": {
        "id": "wdVGq1T-LMdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale returns for fitting the GARCH model\n",
        "scaled_returns = returns * 100\n",
        "\n",
        "# Define a GARCH model with specified p and q parameters and fit it to the data\n",
        "model = arch_model(scaled_returns[:121], p=9, q=7)\n",
        "model_fit = model.fit(disp=\"off\")\n"
      ],
      "metadata": {
        "id": "p58RojcHLT0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the model summary\n",
        "model_fit.summary()\n",
        "\n",
        "# Function to fit GARCH models for given p and q values\n",
        "def fit_garch(scaled_returns, p, q):\n",
        "    model = arch_model(scaled_returns, vol='GARCH', p=p, q=q)\n",
        "    try:\n",
        "        results = model.fit(disp='off')\n",
        "        return results\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Function to check if p-values of fitted model parameters are statistically significant\n",
        "def check_pvalues(results, significance_level=0.05):\n",
        "    if results is None:\n",
        "        return False\n",
        "    params = results.params.iloc[1:]  # Exclude constant\n",
        "    pvalues = results.pvalues.iloc[1:]  # Exclude constant\n",
        "    return any(pvalue < significance_level for pvalue in pvalues)\n",
        "\n",
        "# Find the best GARCH model by iterating over different p and q combinations\n",
        "def find_best_model(scaled_returns, max_p=11, max_q=11, significance_level=0.05):\n",
        "    best_model = None\n",
        "    best_aic = np.inf\n",
        "    best_bic = np.inf\n",
        "    best_pq = None\n",
        "    all_results = {}\n",
        "\n",
        "    # Iterate over combinations of p and q values\n",
        "    for p, q in product(range(1, max_p + 1), range(0, max_q + 1)):\n",
        "        results = fit_garch(scaled_returns, p, q)\n",
        "        all_results[(p, q)] = results\n",
        "\n",
        "        # If results are significant, track the best model based on AIC\n",
        "        if results is not None and check_pvalues(results, significance_level):\n",
        "            if results.aic < best_aic:\n",
        "                best_model = results\n",
        "                best_aic = results.aic\n",
        "                best_bic = results.bic\n",
        "                best_pq = (p, q)\n",
        "\n",
        "    return best_model, best_pq, best_aic, best_bic, all_results\n",
        "\n",
        "# Find and print the best GARCH model based on AIC and BIC criteria\n",
        "best_model, best_pq, best_aic, best_bic, all_results = find_best_model(scaled_returns)"
      ],
      "metadata": {
        "id": "-kkYw2CLLZFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if best_model is not None:\n",
        "    print(f\"Best model: GARCH{best_pq}\")\n",
        "    print(f\"AIC: {best_aic:.4f}\")\n",
        "    print(f\"BIC: {best_bic:.4f}\")\n",
        "    print(\"\\nModel Summary:\")\n",
        "    print(best_model.summary().tables[1])\n",
        "else:\n",
        "    print(\"No suitable model found. Here are the details for all fitted models:\")\n",
        "    for (p, q), results in all_results.items():\n",
        "        if results is not None:\n",
        "            print(f\"\\nGARCH({p},{q}):\")\n",
        "            print(f\"AIC: {results.aic:.4f}\")\n",
        "            print(f\"BIC: {results.bic:.4f}\")\n",
        "            print(\"P-values:\")\n",
        "            print(results.pvalues)\n",
        "        else:\n",
        "            print(f\"\\nGARCH({p},{q}): Failed to converge\")\n",
        "\n"
      ],
      "metadata": {
        "id": "luX9MYcALfpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If a model was found, plot its conditional volatility\n",
        "if best_model is not None:\n",
        "    best_model.plot()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "54OwMY5JLldc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate rolling predictions of volatility over a test size window\n",
        "rolling_predictions = []\n",
        "test_size = 300\n",
        "\n",
        "# Rolling window forecast loop\n",
        "for i in range(test_size):\n",
        "    train = scaled_returns[:-(test_size-i)]\n",
        "    model = arch_model(train, p=9, q=7)\n",
        "    model_fit = model.fit(disp=\"off\")\n",
        "    pred = model_fit.forecast(horizon=1)\n",
        "    rolling_predictions.append(np.sqrt(pred.variance.values[-1,:][0]))\n",
        "\n",
        "# Convert rolling predictions to pandas series and plot\n",
        "rolling_predictions = pd.Series((rolling_predictions), index=scaled_returns.index[-test_size:])\n",
        "plt.figure(figsize=(10,4))\n",
        "true, = plt.plot(scaled_returns[-test_size:])\n",
        "preds, = plt.plot(rolling_predictions)\n",
        "plt.title(\"Volatility - Rolling Forecast\", fontsize=20)\n",
        "plt.legend([\"True Returns\", \"Predicted Volatility\"], fontsize=10)"
      ],
      "metadata": {
        "id": "_L1tKOkcLpFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize percentage changes in rolling predictions\n",
        "plt.figure(figsize=(10,4))\n",
        "rolling_predictions.pct_change().plot()"
      ],
      "metadata": {
        "id": "XMbTjQ53LxGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify spikes in risk (rolling prediction changes above threshold)\n",
        "spike_threshold = 0.25\n",
        "spike_condition = rolling_predictions.pct_change() > spike_threshold\n",
        "# len(spike_condition[spike_condition==True])\n",
        "# spike_condition[spike_condition==True]\n"
      ],
      "metadata": {
        "id": "WtPQM4H6L1aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for trade logic by merging predictions with original sample\n",
        "sample = data[-test_size:]\n",
        "predictions = pd.Series(rolling_predictions, name=\"predictions\")\n",
        "merge = sample.merge(predictions, on=sample.index, how=\"left\")\n",
        "merge[\"pred_chng\"] = merge[\"predictions\"].pct_change()\n",
        "merge.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "dgSwQMB6MF4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty trades DataFrame to store trading decisions\n",
        "trades = pd.DataFrame(columns=['state', 'order_type', 'open_time', 'open_price', 'close_time', 'close_price'])\n",
        "\n",
        "# Trading logic based on volatility spike conditions\n",
        "for i, x in merge.iterrows():\n",
        "    # Open trade logic: open a buy trade if conditions are met\n",
        "    num_open_trades = trades[trades['state'] == 'open'].shape[0]\n",
        "    if (x[\"pred_chng\"] >= 0.25) and (num_open_trades==0):\n",
        "        trades.loc[len(trades), trades.columns] = ['open', 'buy', x['time'], x['open'], None, None]\n",
        "\n",
        "    # Close trade logic: close the trade if price hits high or low conditions\n",
        "    open_trades = trades[trades['state'] == 'open'].shape[0]\n",
        "    if open_trades == 0:\n",
        "        continue\n",
        "\n",
        "    stop_loss = trades[\"open_price\"].iloc[-1] - 2.5\n",
        "    close_cond1 = x[\"high\"] > x[\"open\"]\n",
        "    close_cond2 = x['low'] <= stop_loss\n",
        "\n",
        "    if close_cond1:\n",
        "        trades.loc[trades['state'] == 'open', ['state', 'close_time', 'close_price']] = ['closed', x['time'], x['high']]\n",
        "    if close_cond2:\n",
        "        trades.loc[trades['state'] == 'open', ['state', 'close_time', 'close_price']] = ['closed', x['time'], x['low']]\n",
        "\n"
      ],
      "metadata": {
        "id": "oRyYFW-CMJcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate trade returns and cumulative returns\n",
        "trades[\"returns\"] = trades[\"close_price\"] - trades[\"open_price\"]\n",
        "trades[\"cum_returns\"] = trades[\"returns\"].cumsum()\n",
        "trades = trades[:-1]  # Remove the last incomplete trade\n",
        "\n"
      ],
      "metadata": {
        "id": "JnZgoENUMME6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot cumulative returns over time\n",
        "(trades[\"cum_returns\"]*0.2).plot()"
      ],
      "metadata": {
        "id": "g1UI2Cd5MQGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot candlestick chart of the data\n",
        "candles = go.Candlestick(x=merge[\"time\"],\n",
        "                        open=merge[\"open\"],\n",
        "                        high=merge[\"high\"],\n",
        "                        low=merge[\"low\"],\n",
        "                        close=merge[\"close\"])\n",
        "fig = go.Figure(data=candles)\n",
        "fig.update_layout(xaxis_rangeslider_visible=False, height=700)\n",
        "\n",
        "# Visualize the trades on the candlestick chart\n",
        "for i, trade in trades.iterrows():\n",
        "    color = 'yellow' if trade['returns'] > 0 else 'black'\n",
        "    fig.add_shape(type=\"line\",\n",
        "        x0=trade['open_time'], y0=trade['open_price'], x1=trade['close_time'], y1=trade['close_price'],\n",
        "        line=dict(\n",
        "            color=color,\n",
        "            width=5,\n",
        "            dash=\"dot\",\n",
        "        )\n",
        "    )\n",
        "fig"
      ],
      "metadata": {
        "id": "EEZGekcuMS1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}